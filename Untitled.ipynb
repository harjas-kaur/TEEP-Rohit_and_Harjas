{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "\n",
    "def count_photos_in_folder(folder_path):\n",
    "    # Define common photo file extensions\n",
    "    photo_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff' , '.webp']\n",
    "    \n",
    "    # Initialize a counter\n",
    "    photo_count = 0\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a photo extension\n",
    "        if any(filename.lower().endswith(ext) for ext in photo_extensions):\n",
    "            photo_count += 1\n",
    "    \n",
    "    return photo_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/damaged_infrastructure/images/wreckedcar_2017-02-23_21-15-25.jpg: image file is truncated (0 bytes not processed)\n",
      "Error loading image /home/ai1/Rohit/dataset_2/Damaged_Infrastructure/Infrastructure/05_01_0261.png: image file is truncated (0 bytes not processed)\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/flood/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/flood/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/human_damage/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/human_damage/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/dataset_2/Human_Damage/02_0069.png: cannot identify image file '/home/ai1/Rohit/dataset_2/Human_Damage/02_0069.png'\n",
      "(8432, 128, 128, 3)\n",
      "(2108, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def count_photos_in_folder(folder):\n",
    "    return len([name for name in os.listdir(folder) if os.path.isfile(os.path.join(folder, name))])\n",
    "\n",
    "def load_images_from_folders(folders, target_size=(128, 128)):\n",
    "    images = []\n",
    "    images2 = []\n",
    "    for folder in folders:\n",
    "        i = 0\n",
    "        x = count_photos_in_folder(folder)\n",
    "        x = int(x * 0.8)  # Calculate 80% of the total count\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.resize(target_size)  # Resize image to target size\n",
    "                    if img.mode == 'RGBA':\n",
    "                        img = img.convert('RGB')  # Convert RGBA to RGB\n",
    "                    elif img.mode == 'L':\n",
    "                        img = img.convert('RGB')  # Convert grayscale to RGB\n",
    "                    img_array = np.array(img)\n",
    "                    if img_array.shape == (target_size[0], target_size[1], 3):\n",
    "                        if i < x:\n",
    "                            images.append(img_array)\n",
    "                        else:\n",
    "                            images2.append(img_array)\n",
    "                        i += 1\n",
    "            except (IOError, ValueError) as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                \n",
    "    return images, images2              \n",
    "\n",
    "# Usage example\n",
    "folders = [\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/damaged_infrastructure/images\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Damaged_Infrastructure/Infrastructure\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Land_Disaster/Drought\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Land_Disaster/Land_Slide\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/fires/images\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Fire_Disaster/Urban_Fire\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Fire_Disaster/Wild_Fire\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/flood/images\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Water_Disaster\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/human_damage/images\",\n",
    "    r\"/home/ai1/Rohit/dataset_2/Human_Damage\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/non_damage/images\"\n",
    "]\n",
    "images, images2 = load_images_from_folders(folders)\n",
    "x_train = np.array(images)\n",
    "x_test = np.array(images2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2108, 128, 128, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(2529):\n",
    "    y_train.append(0)\n",
    "for i in range(692):\n",
    "    y_train.append(1)\n",
    "for i in range(1144):\n",
    "    y_train.append(2)    \n",
    "for i in range(1306):\n",
    "    y_train.append(3)\n",
    "for i in range(384):\n",
    "    y_train.append(4)  \n",
    "for i in range(2377):\n",
    "    y_train.append(5)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =[]\n",
    "for i in range(586):\n",
    "    y_test.append(0)\n",
    "for i in range(217):\n",
    "    y_test.append(1)\n",
    "for i in range(287):\n",
    "    y_test.append(2)    \n",
    "for i in range(327):\n",
    "    y_test.append(3)\n",
    "for i in range(96):\n",
    "    y_test.append(4) \n",
    "for i in range(595):\n",
    "    y_test.append(5)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"damaged_infrastructure\" , \"damaged_nature\" , \"fires\" , \"floods\" , \"human_damage\" , \"non_damage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x,y,ind):\n",
    "    plt.imshow(x[ind])\n",
    "    plt.xlabel(classes[y[ind]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train.astype(float)\n",
    "x_test_float = x_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train_float/255\n",
    "x_test_float = x_test_float / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        [0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        [0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        ...,\n",
       "        [0.73333333, 0.88627451, 0.91372549],\n",
       "        [0.73333333, 0.88235294, 0.91372549],\n",
       "        [0.72941176, 0.88235294, 0.90980392]],\n",
       "\n",
       "       [[0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        [0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        [0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        ...,\n",
       "        [0.7372549 , 0.88627451, 0.9254902 ],\n",
       "        [0.73333333, 0.88235294, 0.92156863],\n",
       "        [0.73333333, 0.88235294, 0.92156863]],\n",
       "\n",
       "       [[0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        [0.7254902 , 0.87843137, 0.9372549 ],\n",
       "        [0.7254902 , 0.88235294, 0.93333333],\n",
       "        ...,\n",
       "        [0.73333333, 0.88235294, 0.92941176],\n",
       "        [0.73333333, 0.88235294, 0.92941176],\n",
       "        [0.7372549 , 0.88627451, 0.93333333]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.36078431, 0.1372549 , 0.01176471],\n",
       "        [0.37254902, 0.1372549 , 0.01568627],\n",
       "        [0.37254902, 0.12941176, 0.01176471],\n",
       "        ...,\n",
       "        [0.39215686, 0.1372549 , 0.01176471],\n",
       "        [0.4       , 0.14509804, 0.01960784],\n",
       "        [0.4       , 0.14509804, 0.01960784]],\n",
       "\n",
       "       [[0.36470588, 0.14117647, 0.01176471],\n",
       "        [0.37254902, 0.1372549 , 0.01176471],\n",
       "        [0.37647059, 0.13333333, 0.01176471],\n",
       "        ...,\n",
       "        [0.39215686, 0.1372549 , 0.01176471],\n",
       "        [0.38823529, 0.13333333, 0.00784314],\n",
       "        [0.39215686, 0.1372549 , 0.01568627]],\n",
       "\n",
       "       [[0.35686275, 0.13333333, 0.00784314],\n",
       "        [0.36862745, 0.13333333, 0.01176471],\n",
       "        [0.37254902, 0.12941176, 0.01176471],\n",
       "        ...,\n",
       "        [0.39215686, 0.1372549 , 0.01176471],\n",
       "        [0.38823529, 0.13333333, 0.00784314],\n",
       "        [0.39215686, 0.1372549 , 0.01176471]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_float[507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 16:51:54.413035: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 16:51:54.473800: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 16:51:54.829301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 16:51:57.065087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai1/Rohit/miniconda/envs/rohitenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-14 16:52:01.222133: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters = 32, kernel_size = (3,3) ,activation = 'relu' , input_shape = (128,128,3) ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(filters = 64, kernel_size = (3,3) ,activation = 'relu' , input_shape = (128,128,3) ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax'),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam',\n",
    "           loss = 'sparse_categorical_crossentropy',\n",
    "           metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 190ms/step - accuracy: 0.4216 - loss: 1.5853\n",
      "Epoch 2/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 192ms/step - accuracy: 0.5827 - loss: 1.0901\n",
      "Epoch 3/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 189ms/step - accuracy: 0.7137 - loss: 0.7957\n",
      "Epoch 4/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 187ms/step - accuracy: 0.8357 - loss: 0.4941\n",
      "Epoch 5/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9194 - loss: 0.2595\n",
      "Epoch 6/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 191ms/step - accuracy: 0.9696 - loss: 0.1105\n",
      "Epoch 7/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 192ms/step - accuracy: 0.9832 - loss: 0.0696\n",
      "Epoch 8/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9899 - loss: 0.0426\n",
      "Epoch 9/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 188ms/step - accuracy: 0.9845 - loss: 0.0631\n",
      "Epoch 10/10\n",
      "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 189ms/step - accuracy: 0.9869 - loss: 0.0490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa3a00fe350>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train_float,y_train,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_float.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.5913 - loss: 515.6973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[507.7974853515625, 0.6204933524131775]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains the class labels\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5556873599578226,\n",
       " 1: 2.030828516377649,\n",
       " 2: 1.2284382284382285,\n",
       " 3: 1.076059213884635,\n",
       " 4: 3.6597222222222223,\n",
       " 5: 0.5912214275697658}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted class: fires\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "def classify_image(model, image_path, target_size=(128, 128)):\n",
    "    try:\n",
    "        # Open the image file\n",
    "        img = Image.open(image_path)\n",
    "        # Resize the image to match the input shape expected by the model\n",
    "        img = img.resize(target_size)\n",
    "        # Convert the image to an array\n",
    "        img_array = img_to_array(img)\n",
    "        # Normalize the image array\n",
    "        img_array = img_array / 255.0\n",
    "        # Expand dimensions to match the input shape (batch_size, height, width, channels)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Make predictions using the model\n",
    "        predictions = model.predict(img_array)\n",
    "        # Get the index of the class with the highest confidence\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return predicted_class[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "image_path = r\"/home/ai1/Rohit/download_1.jpeg\"\n",
    "# Assuming you have already trained your model and it's named 'cnn'\n",
    "predicted_class = classify_image(cnn, image_path)\n",
    "print(\"Predicted class:\", classes[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn.save(\"Image_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rohitenv",
   "language": "python",
   "name": "rohitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
