{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "\n",
    "def count_photos_in_folder(folder_path):\n",
    "    # Define common photo file extensions\n",
    "    photo_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff' , '.webp']\n",
    "    \n",
    "    # Initialize a counter\n",
    "    photo_count = 0\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a photo extension\n",
    "        if any(filename.lower().endswith(ext) for ext in photo_extensions):\n",
    "            photo_count += 1\n",
    "    \n",
    "    return photo_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/damaged_infrastructure/images/wreckedcar_2017-02-23_21-15-25.jpg: image file is truncated (0 bytes not processed)\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/flood/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/flood/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/human_damage/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/human_damage/images/.ipynb_checkpoints'\n",
      "(5008, 128, 128, 3)\n",
      "(1252, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def count_photos_in_folder(folder):\n",
    "    return len([name for name in os.listdir(folder) if os.path.isfile(os.path.join(folder, name))])\n",
    "\n",
    "def load_images_from_folders(folders, target_size=(128, 128)):\n",
    "    images = []\n",
    "    images2 = []\n",
    "    for folder in folders:\n",
    "        i = 0\n",
    "        x = count_photos_in_folder(folder)\n",
    "        x = int(x * 0.8)  # Calculate 80% of the total count\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.resize(target_size)  # Resize image to target size\n",
    "                    if img.mode == 'RGBA':\n",
    "                        img = img.convert('RGB')  # Convert RGBA to RGB\n",
    "                    elif img.mode == 'L':\n",
    "                        img = img.convert('RGB')  # Convert grayscale to RGB\n",
    "                    img_array = np.array(img)\n",
    "                    if img_array.shape == (target_size[0], target_size[1], 3):\n",
    "                        if i < x:\n",
    "                            images.append(img_array)\n",
    "                        else:\n",
    "                            images2.append(img_array)\n",
    "                        i += 1\n",
    "            except (IOError, ValueError) as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                \n",
    "    return images, images2              \n",
    "\n",
    "# Usage example\n",
    "folders = [\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/damaged_infrastructure/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/fires/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/flood/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/human_damage/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/non_damage/images\"\n",
    "]\n",
    "images, images2 = load_images_from_folders(folders)\n",
    "x_train = np.array(images)\n",
    "x_test = np.array(images2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5008, 128, 128, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(x_train[1310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(1322):\n",
    "    y_train.append(0)\n",
    "for i in range(241):\n",
    "    y_train.append(1)\n",
    "for i in range(398):\n",
    "    y_train.append(2)    \n",
    "for i in range(478):\n",
    "    y_train.append(3)\n",
    "for i in range(192):\n",
    "    y_train.append(4)  \n",
    "for i in range(2377):\n",
    "    y_train.append(5)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =[]\n",
    "for i in range(296):\n",
    "    y_test.append(0)\n",
    "for i in range(93):\n",
    "    y_test.append(1)\n",
    "for i in range(100):\n",
    "    y_test.append(2)    \n",
    "for i in range(120):\n",
    "    y_test.append(3)\n",
    "for i in range(48):\n",
    "    y_test.append(4) \n",
    "for i in range(595):\n",
    "    y_test.append(5)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"damaged_infrastructure\" , \"damaged_nature\" , \"fires\" , \"floods\" , \"human_damage\" , \"non_damage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x,y,ind):\n",
    "    plt.imshow(x[ind])\n",
    "    plt.xlabel(classes[y[ind]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train.astype(float)\n",
    "x_test_float = x_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train_float/255\n",
    "x_test_float = x_test_float / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.91372549, 0.9372549 , 0.88235294],\n",
       "        [0.91764706, 0.92941176, 0.8745098 ],\n",
       "        [0.61568627, 0.61176471, 0.58431373],\n",
       "        ...,\n",
       "        [0.78039216, 0.78823529, 0.7372549 ],\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333]],\n",
       "\n",
       "       [[0.91372549, 0.94117647, 0.88235294],\n",
       "        [0.86666667, 0.87843137, 0.82352941],\n",
       "        [0.63529412, 0.61176471, 0.59215686],\n",
       "        ...,\n",
       "        [0.78039216, 0.78823529, 0.7372549 ],\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333]],\n",
       "\n",
       "       [[0.90980392, 0.93333333, 0.87843137],\n",
       "        [0.79607843, 0.79215686, 0.74117647],\n",
       "        [0.75294118, 0.73333333, 0.70980392],\n",
       "        ...,\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        ...,\n",
       "        [0.17647059, 0.14901961, 0.18039216],\n",
       "        [0.18039216, 0.14901961, 0.18431373],\n",
       "        [0.18823529, 0.14901961, 0.18431373]],\n",
       "\n",
       "       [[0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        ...,\n",
       "        [0.16078431, 0.13333333, 0.16470588],\n",
       "        [0.15294118, 0.12941176, 0.16078431],\n",
       "        [0.14509804, 0.12941176, 0.15686275]],\n",
       "\n",
       "       [[0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10196078, 0.09411765, 0.1372549 ],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        ...,\n",
       "        [0.1254902 , 0.11372549, 0.14901961],\n",
       "        [0.12156863, 0.10588235, 0.14117647],\n",
       "        [0.11372549, 0.10588235, 0.1372549 ]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_float[507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 15:52:05.553005: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-12 15:52:05.619501: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-12 15:52:05.961282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 15:52:07.877383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai1/Rohit/miniconda/envs/rohitenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-12 15:52:09.978748: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters = 32, kernel_size = (3,3) ,activation = 'relu' , input_shape = (128,128,3) ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(filters = 64, kernel_size = (3,3) ,activation = 'relu' , input_shape = (128,128,3) ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax'),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam',\n",
    "           loss = 'sparse_categorical_crossentropy',\n",
    "           metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 187ms/step - accuracy: 0.4624 - loss: 1.6464\n",
      "Epoch 2/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 188ms/step - accuracy: 0.6010 - loss: 1.0442\n",
      "Epoch 3/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 186ms/step - accuracy: 0.6770 - loss: 0.8624\n",
      "Epoch 4/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 188ms/step - accuracy: 0.7540 - loss: 0.6553\n",
      "Epoch 5/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 187ms/step - accuracy: 0.8785 - loss: 0.3843\n",
      "Epoch 6/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 188ms/step - accuracy: 0.9313 - loss: 0.2292\n",
      "Epoch 7/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 188ms/step - accuracy: 0.9645 - loss: 0.1350\n",
      "Epoch 8/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 189ms/step - accuracy: 0.9804 - loss: 0.0788\n",
      "Epoch 9/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 191ms/step - accuracy: 0.9928 - loss: 0.0348\n",
      "Epoch 10/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 187ms/step - accuracy: 0.9938 - loss: 0.0279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fc655c61b50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train_float,y_train,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_float.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5568 - loss: 3.2150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.973661422729492, 0.5750798583030701]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(x_test_float , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99999994, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99999994, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.99999994, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.99999994, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.99999994, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = cnn.predict(x_test)\n",
    "y_predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes = [np.argmax(element) for element in y_predict]\n",
    "y_classes[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains the class labels\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6313666162380231,\n",
       " 1: 3.463347164591978,\n",
       " 2: 2.09715242881072,\n",
       " 3: 1.7461645746164574,\n",
       " 4: 4.347222222222222,\n",
       " 5: 0.35114289720936753}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=/home/ai1/Rohit/casualtie.jpeg (of type <class 'str'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ai1/Rohit/casualtie.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Assuming you have already trained your model and it's named 'cnn'\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Rohit/miniconda/envs/rohitenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Rohit/miniconda/envs/rohitenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/__init__.py:120\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x=/home/ai1/Rohit/casualtie.jpeg (of type <class 'str'>)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "def classify_image(model, image_path, target_size=(128, 128)):\n",
    "    try:\n",
    "        # Open the image file\n",
    "        img = Image.open(image_path)\n",
    "        # Resize the image to match the input shape expected by the model\n",
    "        img = img.resize(target_size)\n",
    "        # Convert the image to an array\n",
    "        img_array = img_to_array(img)\n",
    "        # Normalize the image array\n",
    "        img_array = img_array / 255.0\n",
    "        # Expand dimensions to match the input shape (batch_size, height, width, channels)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Make predictions using the model\n",
    "        predictions = model.predict(img_array)\n",
    "        # Get the index of the class with the highest confidence\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return predicted_class[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "image_path = r\"/home/ai1/Rohit/casualtie.jpeg\"\n",
    "# Assuming you have already trained your model and it's named 'cnn'\n",
    "predicted_class = cnn.predict(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rohitenv",
   "language": "python",
   "name": "rohitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
