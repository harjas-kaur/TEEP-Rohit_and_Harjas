{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "\n",
    "def count_photos_in_folder(folder_path):\n",
    "    # Define common photo file extensions\n",
    "    photo_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff' , '.webp']\n",
    "    \n",
    "    # Initialize a counter\n",
    "    photo_count = 0\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a photo extension\n",
    "        if any(filename.lower().endswith(ext) for ext in photo_extensions):\n",
    "            photo_count += 1\n",
    "    \n",
    "    return photo_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/damaged_infrastructure/images/wreckedcar_2017-02-23_21-15-25.jpg: image file is truncated (0 bytes not processed)\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/flood/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/flood/images/.ipynb_checkpoints'\n",
      "Error loading image /home/ai1/Rohit/multimodalitie/multimodal/human_damage/images/.ipynb_checkpoints: [Errno 21] Is a directory: '/home/ai1/Rohit/multimodalitie/multimodal/human_damage/images/.ipynb_checkpoints'\n",
      "(5008, 128, 128, 3)\n",
      "(1252, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def count_photos_in_folder(folder):\n",
    "    return len([name for name in os.listdir(folder) if os.path.isfile(os.path.join(folder, name))])\n",
    "\n",
    "def load_images_from_folders(folders, target_size=(128, 128)):\n",
    "    images = []\n",
    "    images2 = []\n",
    "    for folder in folders:\n",
    "        i = 0\n",
    "        x = count_photos_in_folder(folder)\n",
    "        x = int(x * 0.8)  # Calculate 80% of the total count\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.resize(target_size)  # Resize image to target size\n",
    "                    if img.mode == 'RGBA':\n",
    "                        img = img.convert('RGB')  # Convert RGBA to RGB\n",
    "                    elif img.mode == 'L':\n",
    "                        img = img.convert('RGB')  # Convert grayscale to RGB\n",
    "                    img_array = np.array(img)\n",
    "                    if img_array.shape == (target_size[0], target_size[1], 3):\n",
    "                        if i < x:\n",
    "                            images.append(img_array)\n",
    "                        else:\n",
    "                            images2.append(img_array)\n",
    "                        i += 1\n",
    "            except (IOError, ValueError) as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                \n",
    "    return images, images2              \n",
    "\n",
    "# Usage example\n",
    "folders = [\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/damaged_infrastructure/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/damaged_nature/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/fires/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/flood/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/human_damage/images\",\n",
    "    r\"/home/ai1/Rohit/multimodalitie/multimodal/non_damage/images\"\n",
    "]\n",
    "images, images2 = load_images_from_folders(folders)\n",
    "x_train = np.array(images)\n",
    "x_test = np.array(images2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5008, 128, 128, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(x_train[1310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(1322):\n",
    "    y_train.append(0)\n",
    "for i in range(241):\n",
    "    y_train.append(1)\n",
    "for i in range(398):\n",
    "    y_train.append(2)    \n",
    "for i in range(478):\n",
    "    y_train.append(3)\n",
    "for i in range(192):\n",
    "    y_train.append(4)  \n",
    "for i in range(2377):\n",
    "    y_train.append(5)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =[]\n",
    "for i in range(296):\n",
    "    y_test.append(0)\n",
    "for i in range(93):\n",
    "    y_test.append(1)\n",
    "for i in range(100):\n",
    "    y_test.append(2)    \n",
    "for i in range(120):\n",
    "    y_test.append(3)\n",
    "for i in range(48):\n",
    "    y_test.append(4) \n",
    "for i in range(595):\n",
    "    y_test.append(5)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"damaged_infrastructure\" , \"damaged_nature\" , \"fires\" , \"floods\" , \"human_damage\" , \"non_damage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x,y,ind):\n",
    "    plt.imshow(x[ind])\n",
    "    plt.xlabel(classes[y[ind]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train.astype(float)\n",
    "x_test_float = x_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train_float/255\n",
    "x_test_float = x_test_float / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.91372549, 0.9372549 , 0.88235294],\n",
       "        [0.91764706, 0.92941176, 0.8745098 ],\n",
       "        [0.61568627, 0.61176471, 0.58431373],\n",
       "        ...,\n",
       "        [0.78039216, 0.78823529, 0.7372549 ],\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333]],\n",
       "\n",
       "       [[0.91372549, 0.94117647, 0.88235294],\n",
       "        [0.86666667, 0.87843137, 0.82352941],\n",
       "        [0.63529412, 0.61176471, 0.59215686],\n",
       "        ...,\n",
       "        [0.78039216, 0.78823529, 0.7372549 ],\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333]],\n",
       "\n",
       "       [[0.90980392, 0.93333333, 0.87843137],\n",
       "        [0.79607843, 0.79215686, 0.74117647],\n",
       "        [0.75294118, 0.73333333, 0.70980392],\n",
       "        ...,\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333],\n",
       "        [0.77647059, 0.78431373, 0.73333333]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        ...,\n",
       "        [0.17647059, 0.14901961, 0.18039216],\n",
       "        [0.18039216, 0.14901961, 0.18431373],\n",
       "        [0.18823529, 0.14901961, 0.18431373]],\n",
       "\n",
       "       [[0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        ...,\n",
       "        [0.16078431, 0.13333333, 0.16470588],\n",
       "        [0.15294118, 0.12941176, 0.16078431],\n",
       "        [0.14509804, 0.12941176, 0.15686275]],\n",
       "\n",
       "       [[0.10588235, 0.09803922, 0.14117647],\n",
       "        [0.10196078, 0.09411765, 0.1372549 ],\n",
       "        [0.10588235, 0.09803922, 0.14117647],\n",
       "        ...,\n",
       "        [0.1254902 , 0.11372549, 0.14901961],\n",
       "        [0.12156863, 0.10588235, 0.14117647],\n",
       "        [0.11372549, 0.10588235, 0.1372549 ]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_float[507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:17:03.812318: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-13 11:17:03.857937: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-13 11:17:04.115538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 11:17:05.473541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai1/Rohit/miniconda/envs/rohitenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-13 11:17:06.843452: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters = 32, kernel_size = (3,3) ,activation = 'relu' , input_shape = (128,128,3) ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(filters = 64, kernel_size = (3,3) ,activation = 'relu' , input_shape = (128,128,3) ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax'),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam',\n",
    "           loss = 'sparse_categorical_crossentropy',\n",
    "           metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 202ms/step - accuracy: 0.4610 - loss: 1.6490\n",
      "Epoch 2/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.6201 - loss: 1.0426\n",
      "Epoch 3/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.6541 - loss: 0.9123\n",
      "Epoch 4/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.7213 - loss: 0.7274\n",
      "Epoch 5/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 202ms/step - accuracy: 0.8253 - loss: 0.5142\n",
      "Epoch 6/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 202ms/step - accuracy: 0.8942 - loss: 0.3199\n",
      "Epoch 7/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.9390 - loss: 0.1920\n",
      "Epoch 8/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.9626 - loss: 0.1211\n",
      "Epoch 9/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 203ms/step - accuracy: 0.9792 - loss: 0.0877\n",
      "Epoch 10/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 201ms/step - accuracy: 0.9911 - loss: 0.0518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f6587c226d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train_float,y_train,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_float.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.4310 - loss: 3.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8615851402282715, 0.5742811560630798]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains the class labels\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6313666162380231,\n",
       " 1: 3.463347164591978,\n",
       " 2: 2.09715242881072,\n",
       " 3: 1.7461645746164574,\n",
       " 4: 4.347222222222222,\n",
       " 5: 0.35114289720936753}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted class: damaged_infrastructure\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "def classify_image(model, image_path, target_size=(128, 128)):\n",
    "    try:\n",
    "        # Open the image file\n",
    "        img = Image.open(image_path)\n",
    "        # Resize the image to match the input shape expected by the model\n",
    "        img = img.resize(target_size)\n",
    "        # Convert the image to an array\n",
    "        img_array = img_to_array(img)\n",
    "        # Normalize the image array\n",
    "        img_array = img_array / 255.0\n",
    "        # Expand dimensions to match the input shape (batch_size, height, width, channels)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Make predictions using the model\n",
    "        predictions = model.predict(img_array)\n",
    "        # Get the index of the class with the highest confidence\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return predicted_class[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "image_path = r\"/home/ai1/Rohit/flood_2.jpeg\"\n",
    "# Assuming you have already trained your model and it's named 'cnn'\n",
    "predicted_class = classify_image(cnn, image_path)\n",
    "print(\"Predicted class:\", classes[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn.save(\"image_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rohitenv",
   "language": "python",
   "name": "rohitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
